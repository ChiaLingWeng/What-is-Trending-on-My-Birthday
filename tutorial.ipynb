{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\37604\\AppData\\Local\\Temp\\ipykernel_24848\\3678392199.py:48: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  if BS(rs.get(base_index[:-5] + str(start_index) + \".html\").text, \"html.parser\").find(text=\"500 - Internal Server Error\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- start index 39217 ----\n",
      "\n",
      "no.1 https://www.ptt.cc/bbs/Gossiping/M.1697698133.A.0AC.html ok\n",
      "no.2 https://www.ptt.cc/bbs/Gossiping/M.1697698166.A.311.html ok\n",
      "no.3 https://www.ptt.cc/bbs/Gossiping/M.1697698174.A.DF7.html ok\n",
      "no.4 https://www.ptt.cc/bbs/Gossiping/M.1697698187.A.4E7.html ok\n",
      "no.5 https://www.ptt.cc/bbs/Gossiping/M.1697698269.A.34B.html ok\n",
      "no.6 https://www.ptt.cc/bbs/Gossiping/M.1697698290.A.1FB.html ok\n",
      "no.7 https://www.ptt.cc/bbs/Gossiping/M.1697698294.A.E1B.html ok\n",
      "no.8 https://www.ptt.cc/bbs/Gossiping/M.1694428619.A.DBA.html ok\n",
      "no.9 https://www.ptt.cc/bbs/Gossiping/M.1697045578.A.710.html ok\n",
      "no.10 https://www.ptt.cc/bbs/Gossiping/M.1697435480.A.6CB.html ok\n",
      "no.11 https://www.ptt.cc/bbs/Gossiping/M.1697522392.A.180.html ok\n",
      "no.12 https://www.ptt.cc/bbs/Gossiping/M.1697547780.A.934.html ok\n",
      "\n",
      "---- finish index 39217 ----\n",
      "\n",
      "==== Complete! ====\n",
      "6.28 sec\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "'''\n",
    "$ python ptt_crawler.py gossiping 10000 9999 ptt\n",
    "'''\n",
    "import sys\n",
    "import urllib\n",
    "import json\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "# POST 給驗證年齡的資料\n",
    "LOAD = {\n",
    "    \"from\": \"/bbs/Gossiping/index.html\",\n",
    "    \"yes\": \"yes\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_all_articles(base_index, start_pages, pages):\n",
    "    '''\n",
    "\n",
    "    base_index: 看板的原始網址 (e.g. \"https://www.ptt.cc/bbs/Gossiping/index.html\")\n",
    "    start_pages: 從倒數第幾頁開始, 1 = 從最後一頁開始抓\n",
    "    pages: 總共抓幾頁, 向最後一頁前進\n",
    "    (base,5,2): 從倒數第五頁開始抓兩頁\n",
    "    return: Article list\n",
    "    '''\n",
    "\n",
    "    if pages > start_pages:\n",
    "        print(\"pages error!\")\n",
    "        return None\n",
    "    rs = requests.session()\n",
    "    re = rs.get(base_index)\n",
    "\n",
    "    # 處理 18 禁同意頁面\n",
    "    if \"over18\" in re.url:\n",
    "        rs.post(\"https://www.ptt.cc/ask/over18\", data=LOAD)\n",
    "\n",
    "    re = rs.get(base_index)\n",
    "    soup = BS(re.text, \"html.parser\")  # parse a document by XML parser\n",
    "\n",
    "    # 利用網頁內 \"上頁\" 按鈕的網址來得知當前 index\n",
    "    prev_page = soup.find_all(\"a\", \"btn wide\")[1].get(\"href\")\n",
    "    prev_index = prev_page[(prev_page.find(\"index\") + 5): prev_page.find(\".html\")]\n",
    "    start_index = int(prev_index) + 1 - (start_pages - 1)\n",
    "    # 有時會有上一頁剛好是最後一頁的情形, 此時算出來的要再減一\n",
    "    if BS(rs.get(base_index[:-5] + str(start_index) + \".html\").text, \"html.parser\").find(text=\"500 - Internal Server Error\"):\n",
    "        start_index -= 1\n",
    "\n",
    "    article_list = []\n",
    "\n",
    "    # 先存下全部所要頁面的 index 數字 (.../index12345.html)\n",
    "    index_list = [i for i in range(start_index, start_index + pages)]\n",
    "    no = 1\n",
    "\n",
    "    comma = False\n",
    "    for idx in index_list:\n",
    "\n",
    "        cur_url = base_index[:-5] + str(idx) + \".html\"\n",
    "        re = rs.get(cur_url)\n",
    "        soup = BS(re.text, \"html.parser\")\n",
    "\n",
    "        bs_title_list = soup.find_all(\"div\", \"r-ent\")  # 獲取標題列表,用來進去各篇文章\n",
    "\n",
    "        print(\"---- start index {0} ----\\n\".format(idx))\n",
    "\n",
    "        for ar in bs_title_list:\n",
    "            title_link = ar.find(\"a\")\n",
    "            # 不合規格的文章判斷(如:\"本文已被刪除\")\n",
    "            if title_link:\n",
    "                title_link = title_link.get(\"href\")  # 文章網址\n",
    "                url = urllib.parse.urljoin(cur_url, title_link)\n",
    "                article_data = get_article(rs.get(url))  # dict\n",
    "                if article_data:\n",
    "                    print(\"no.{0} {1} ok\".format(no, url))\n",
    "                    article_data[\"a_no\"] = no\n",
    "                    json_data = (\",\" if comma else \"\") + json.dumps(article_data, ensure_ascii=False, indent=4,\n",
    "                                                                    sort_keys=True)\n",
    "                    article_list.append(json_data)\n",
    "                    if not comma:\n",
    "                        comma = True\n",
    "\n",
    "                    no += 1\n",
    "\n",
    "            # 避免被當作攻擊\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        print(\"\\n---- finish index {0} ----\\n\".format(idx))\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    rs.close()\n",
    "\n",
    "    return start_index, article_list\n",
    "\n",
    "\n",
    "def get_article(re):\n",
    "    soup = BS(re.content, \"html.parser\")\n",
    "    metalines = soup.find_all(\"div\", \"article-metaline\")\n",
    "\n",
    "    try:\n",
    "        # 文章作者\n",
    "        author = check_data(metalines[0].find(\"span\", \"article-meta-value\"))\n",
    "\n",
    "        # 文章標題\n",
    "        title = check_data(metalines[1].find(\"span\", \"article-meta-value\"))\n",
    "\n",
    "        # 文章時間\n",
    "        a_time = check_data(metalines[2].find(\"span\", \"article-meta-value\"))\n",
    "    except Exception as e:\n",
    "        print(\"error infomation(e.g. author,title,time) at\", re.url)\n",
    "        print(repr(e))\n",
    "        return None\n",
    "\n",
    "    # 文章內容\n",
    "    try:\n",
    "\n",
    "        bs_main_content = soup.find(\"div\", id=\"main-content\")\n",
    "        # 利用時間和文章結尾來做分割\n",
    "        sp1 = bs_main_content.get_text().split(\"--\\n※ 發信站\")\n",
    "        sp2 = sp1[0].split(a_time)\n",
    "        content = sp2[1]\n",
    "    except Exception as e:\n",
    "        print(\"error content at\", re.url)\n",
    "        print(repr(e))\n",
    "        return None\n",
    "\n",
    "    # 回應內容\n",
    "    good = 0\n",
    "    boo = 0\n",
    "    arrow = 0\n",
    "    bs_comments = soup.find_all(\"div\", \"push\")\n",
    "    comments = []\n",
    "    if bs_comments:\n",
    "        for c in bs_comments:\n",
    "\n",
    "            # 處理 \"檔案過大！部分文章無法顯示\" 的 \"warning-box\" class\n",
    "            if \"warning-box\" in c.get(\"class\"):\n",
    "                continue\n",
    "\n",
    "            c_type = c.find(\"span\", class_=\"push-tag\").get_text().strip()\n",
    "            if c_type == \"→\":\n",
    "                arrow += 1\n",
    "            elif c_type == \"推\":\n",
    "                good += 1\n",
    "            elif c_type == \"噓\":\n",
    "                boo += 1\n",
    "            c_id = c.find(\"span\", class_=\"push-userid\").get_text()\n",
    "            c_content = c.find(\"span\", class_=\"push-content\").get_text()\n",
    "\n",
    "            comments.append({\"a_id\": c_id, \"b_type\": c_type, \"c_content\": c_content.strip(\": \")})\n",
    "\n",
    "    statistics = {\"a_total\": good - boo, \"b_good\": good, \"c_boo\": boo, \"d_arrow\": arrow}\n",
    "    data = {\"b_title\": title, \"c_author\": author, \"d_content\": content, \"e_comments\": comments,\n",
    "            \"f_statistics\": statistics,\n",
    "            \"g_url\": re.url}\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def check_data(bs_tag):\n",
    "    if bs_tag:\n",
    "        return bs_tag.get_text()\n",
    "    else:\n",
    "        print(\"format error\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main(board, start_pages, pages, filename):\n",
    "    base_index = \"https://www.ptt.cc/bbs/\" + board + \"/index.html\"\n",
    "    LOAD[\"form\"] = \"/bbs/\" + board + \"/index.html\"\n",
    "\n",
    "    start_index, article_list = get_all_articles(base_index, start_pages, pages)  # return (start_index,string list)\n",
    "\n",
    "    # open 時要以 UTF-8 開啟,否則在 windows 下以 cp950 去做解碼會有誤\n",
    "    with open(\"{0}_pages_{1}_start_index_{2}.json\".format(filename, pages, start_index), \"w\", encoding=\"UTF-8\") as f:\n",
    "        f.write(\"[\\n\")  # json array\n",
    "        for ar in article_list:\n",
    "            f.write(ar)\n",
    "        f.write(\"\\n]\")\n",
    "\n",
    "    print(\"==== Complete! ====\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sec = time.time()\n",
    "    main(\"gossiping\",1,1,\"ptt\")\n",
    "    print(\"{0:.2f} sec\".format(time.time() - sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
